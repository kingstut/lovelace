# Neural Compiler for Deep Learning Optimization and Scheduling

## Overview

This project aims to develop a neural network-based approach to optimize and schedule deep learning models, mimicking the functionality of traditional compiler frameworks like TVM. Instead of relying on hand-crafted heuristics, our neural compiler learns optimization and scheduling strategies from data, potentially uncovering novel techniques and patterns.

## Features

- **Input**: Relay Intermediate Representation (IR) of deep learning models.
- **Output**: Scheduled Tensor Expressions (TEs) ready for backend code generation.
- **Training Data**: Pairs of Relay IRs and their corresponding scheduled TEs generated by TVM.

## How It Works

1. **Data Collection**: Use TVM to generate a dataset of Relay IRs and their corresponding scheduled TEs.
2. **Neural Network Training**: Train a neural network to map Relay IRs to scheduled TEs. The network learns to understand the semantics of the operations, apply common optimization patterns, and decide on effective scheduling strategies.
3. **Inference**: Given a new Relay IR, the neural network predicts the optimized and scheduled TE.

## Getting Started

### Prerequisites

- Python 3.x
- PyTorch
- TVM (for data collection and backend code generation)


## Workflow:
- Model Definition and Conversion: Start by defining a PyTorch model and converting it to ONNX using. See model_templates/mlp.py  
- ONNX to TVM IRs: Convert the ONNX model to Relay IR and Tensor IR using get_tvm_ir.py.  
- Data Preparation: Use nn/parse.py to load and preprocess the Relay IR and Tensor IR data.  
- Neural Network Training: Train the neural network using nn/train.py.  